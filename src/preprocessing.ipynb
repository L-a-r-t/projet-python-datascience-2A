{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_Note préliminaire_ : Il est possible d'utiliser cudf.pandas pour accélérer les opérations tabulaires de Pandas sur GPU. La cellule ci-dessous est facultative, mais (une fois le code décommenté) elle installe et charge cuDF pour **CUDA 12** (version de CUDA utilisée par les GPU du Datalab SSP Cloud). Pour les versions adaptées à CUDA 11, se référer à la [documentation RAPIDS](https://docs.rapids.ai/install/)."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T02:11:49.414640Z",
     "start_time": "2024-12-30T02:11:49.408640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12==24.12.* dask-cudf-cu12==24.12.* cuml-cu12==24.12.* cugraph-cu12==24.12.*\n",
    "\n",
    "# %load_ext cudf.pandas"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La cellule ci-dessous installe les dépendances du projet."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T02:11:53.789065Z",
     "start_time": "2024-12-30T02:11:50.827160Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -q -r ../requirements.txt",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pré-traitement des données\n",
    "\n",
    "Les données issues du scraping de l'API IDF Mobilités ne constituent pas un jeu de données en tant que tel. Le rôle de ce notebook est de transformer ces données brutes, stockées sur le SSP Cloud, en un jeu de données exploitable. Les fichiers JSON contiennent des informations sur les perturbations du réseau francilien, comme leurs causes, gravités et périodes d'application (il faut noter la quantité d'information donnée par l'API — les champs disponibles dans la réponse — varie d'une requête à une autre). Ces perturbations impactent certains objets, entités spécifiques du réseau : lignes de transport, ou arrêts sur ces lignes, permettant de localiser précisément les perturbations.\n",
    "\n",
    "On commence par importer les modules dont on aura besoin (notamment la configuration, json, pandas, tqdm pour avoir un suivi de la progression des opérations) et lister les chemins des fichiers JSON sur le S3 (résultats du scraping période)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T02:40:04.671280Z",
     "start_time": "2024-12-30T02:40:04.663812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import config\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Système de fichiers S3 configuré via config\n",
    "fs = config.fs\n",
    "\n",
    "# Récupération des chemins des fichiers .json dans le stockage S3\n",
    "file_paths = [fp for fp in fs.ls(config.MINIO_ROOT) if fp.endswith(\".json\")]\n",
    "\n",
    "# Initialisation des structures de données\n",
    "all_results = []  # Stocke toutes les dates de la requête\n",
    "all_disruptions = []  # Stocke tous les ID de perturbations\n",
    "all_objects = []  # Stocke tous les ID des objets impactés\n",
    "\n",
    "# Données finales à transformer en DataFrame\n",
    "results_data = []\n",
    "disruptions_data = []\n",
    "objects_data = []\n",
    "objects_disruptions_data = []\n",
    "\n",
    "# Ensembles pour éviter les doublons\n",
    "seen_results = set()              # last_updated\n",
    "seen_disruptions = set()          # (disruption_id, begin, end)\n",
    "seen_objects = set()              # object_id\n",
    "seen_objects_disruptions = set()  # (object_id, disruption_id)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On va ensuite lire ces fichiers, filtrer les doublons, et structurer les données en trois tables principales : une pour les perturbations, une pour les objets impactés (lignes, stations), et une faisant la jointure entre les deux : un lien objet-perturbation, correspondant à une perturbation unique sur une période unique et pour un objet impacté unique. Ainsi, une perturbation sur deux périodes et impactant 3 objets (par exemple des travaux sur deux week-ends et affectant 3 lignes) correspondra à 6 liens objet-perturbation."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T03:00:11.557945Z",
     "start_time": "2024-12-30T02:40:06.985320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with tqdm(total=len(file_paths), desc=\"Traitement des fichiers JSON\", unit=\"fichier\") as pbar:\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Lecture du fichier JSON\n",
    "            with fs.open(file_path, \"r\", encoding=\"ascii\") as f:\n",
    "                data = json.loads(f.read())\n",
    "\n",
    "                # Extraction de la date de la requête\n",
    "                last_updated = data.get(\"lastUpdatedDate\", None)\n",
    "\n",
    "                all_results.append(last_updated)\n",
    "                if last_updated not in seen_results:\n",
    "                    seen_results.add(last_updated)\n",
    "                    results_data.append(last_updated)\n",
    "\n",
    "                # Traitement des perturbations\n",
    "                for d in data.get(\"disruptions\", []):\n",
    "                    disruption_id = d.get(\"id\")\n",
    "                    all_disruptions.append(disruption_id)\n",
    "                    for p in d.get(\"applicationPeriods\", []):\n",
    "                        key = (disruption_id, p.get(\"begin\"), p.get(\"end\"))\n",
    "                        if key not in seen_disruptions:\n",
    "                            seen_disruptions.add(key)\n",
    "                            disruptions_data.append({\n",
    "                                \"disruption_id\": disruption_id,\n",
    "                                \"begin\": p.get(\"begin\"),\n",
    "                                \"end\": p.get(\"end\"),\n",
    "                                \"lastUpdate\": d.get(\"lastUpdate\"),\n",
    "                                \"cause\": d.get(\"cause\"),\n",
    "                                \"severity\": d.get(\"severity\"),\n",
    "                                \"title\": d.get(\"title\"),\n",
    "                                \"message\": d.get(\"message\"),\n",
    "                                \"file_lastUpdatedDate\": last_updated,\n",
    "                            })\n",
    "\n",
    "                # Traitement des lignes et objets impactés\n",
    "                for l in data.get(\"lines\", []):\n",
    "                    line_info = {\n",
    "                        \"line_id\": l.get(\"id\"),\n",
    "                        \"line_name\": l.get(\"name\"),\n",
    "                        \"line_shortName\": l.get(\"shortName\"),\n",
    "                        \"line_mode\": l.get(\"mode\"),\n",
    "                        \"line_networkId\": l.get(\"networkId\"),\n",
    "                        \"file_lastUpdatedDate\": last_updated,\n",
    "                    }\n",
    "                    for o in l.get(\"impactedObjects\", []):\n",
    "                        object_id = o.get(\"id\")\n",
    "                        all_objects.append(object_id)\n",
    "                        if object_id not in seen_objects:\n",
    "                            seen_objects.add(object_id)\n",
    "                            objects_data.append({\n",
    "                                **line_info,\n",
    "                                \"object_id\": object_id,\n",
    "                                \"object_name\": o.get(\"name\"),\n",
    "                                \"object_type\": o.get(\"type\"),\n",
    "                            })\n",
    "\n",
    "                        for disruption_id in o.get(\"disruptionIds\", []):\n",
    "                            key = (object_id, disruption_id)\n",
    "                            if key not in seen_objects_disruptions:\n",
    "                                seen_objects_disruptions.add(key)\n",
    "                                objects_disruptions_data.append({\n",
    "                                    **line_info,\n",
    "                                    \"object_id\": object_id,\n",
    "                                    \"object_name\": o.get(\"name\"),\n",
    "                                    \"object_type\": o.get(\"type\"),\n",
    "                                    \"disruption_id\": disruption_id,\n",
    "                                })\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur est survenue avec le fichier : {file_path}\")\n",
    "            raise e\n",
    "        finally:\n",
    "            pbar.update(1)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des fichiers JSON: 100%|██████████| 2357/2357 [20:04<00:00,  1.96fichier/s] \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Les données ainsi nettoyées sont converties en DataFrames pandas. Créer des DataFrames plus rudimentaires puis les raffiner s'avère être un processus très complexe du fait d'objets imbriqués et de listes de longueurs variables dans nos données brutes. On affiche les comptes d'entités (doublons compris et éliminés) traités."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T03:12:49.495619Z",
     "start_time": "2024-12-30T03:12:49.181575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_disruptions = pd.DataFrame(disruptions_data)\n",
    "df_objects = pd.DataFrame(objects_data)\n",
    "df_objects_disruptions = pd.DataFrame(objects_disruptions_data)\n",
    "\n",
    "# Résumé final\n",
    "print(\"Résultats totaux (tous) :\", len(all_results))\n",
    "print(\"Total de perturbations traitées (toutes) :\", len(all_disruptions))\n",
    "print(\"Total d'objets impactés traités (tous) :\", len(all_objects))\n",
    "\n",
    "print(\"#####################\")\n",
    "\n",
    "print(\"Résultats totaux (sans doublons) :\", len(results_data))\n",
    "print(\"Total de perturbations traitées (sans doublons):\", len(disruptions_data))\n",
    "print(\"Total d'objets impactés traités (sans doublons) :\", len(objects_data))\n",
    "print(\"Total de liens objet-perturbation traités :\", len(objects_disruptions_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats totaux (tous) : 2357\n",
      "Total de perturbations traitées (toutes) : 1724401\n",
      "Total d'objets impactés traités (tous) : 5476570\n",
      "#####################\n",
      "Résultats totaux (sans doublons) : 2357\n",
      "Total de perturbations traitées (sans doublons): 30177\n",
      "Total d'objets impactés traités (sans doublons) : 7570\n",
      "Total de liens objet-perturbation traités : 102807\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On comprend ici que l'API IDF Mobilités est mise à jour plus régulièrement que notre fréquence de scraping (aucun doublon dans les résultats des appels API). Cela veut dire qu'il n'est pas impossible que nous ayons manqué des perturbations de très courte durée sur la période considérée. Nous garderons cela en tête pour l'analyse des données.\n",
    "\n",
    "La déduplication a toutefois été très importante pour les données sur les perturbations et sur les lignes, ce qui était attendu. Avec 19757 perturbations différentes dans notre jeu de données établi sur 3 semaines. Nous avons assez de données pour faire une analyse intéressante, bien que l'idéal serait de produire un outil permettant une analyse continue et automatisée des perturbations fournies par l'API. Avant tout, enregistrons ce jeu de données fraîchement généré."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T03:13:55.872275Z",
     "start_time": "2024-12-30T03:13:55.482275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_disruptions.to_feather(\"data/disruptions.feather\")\n",
    "df_objects.to_feather(\"data/objects.feather\")\n",
    "df_objects_disruptions.to_feather(\"data/objects_disruptions.feather\")"
   ],
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
