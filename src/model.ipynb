{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907afd2ae5ee55e2",
   "metadata": {},
   "source": [
    "_Note préliminaire :_ La dernière session d'entraînement a été exécutée avec ce notebook sur une instance avec un GPU L4. Le notebook est cependant conçu pour être éxécuté avec l'environnement jupyter-pytorch-gpu du SSP Cloud, et c'est d'ailleurs avec cet environnement que nous avons fait la plupart des tests. De nombreuses dépendances y sont présentes et ne sont pas explicitement installées dans ce notebook. La cellule ci-dessous installe les dépendances supplémentaires. Du fait de l'installation de SentencePiece, il faut **impérativement redémarrer** le kernel jupyter après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3986afce16f7b60c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/jovyan/ai/.venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /home/jovyan/ai/.venv/lib/python3.10/site-packages (1.26.1)\n",
      "Requirement already satisfied: html2text in /home/jovyan/ai/.venv/lib/python3.10/site-packages (2024.2.26)\n",
      "Requirement already satisfied: torch in /home/jovyan/ai/.venv/lib/python3.10/site-packages (2.1.0+cu121)\n",
      "Requirement already satisfied: transformers in /home/jovyan/ai/.venv/lib/python3.10/site-packages (4.35.0)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/ai/.venv/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: SentencePiece in /home/jovyan/ai/.venv/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: pyarrow in /home/jovyan/ai/.venv/lib/python3.10/site-packages (18.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jovyan/ai/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy html2text torch transformers tqdm SentencePiece pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07386460419d76e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Entrainement et évaluation du modèle\n",
    "\n",
    "Ce notebook détaille l'entraînement du modèle. On travaille avec le jeu de données des liens-perturbations objets. L'objectif est d'entraîner un modèle pour prédire \n",
    "la durée de l'incident à partir du message d'alerte, tout en utilisant les données catégoriques supplémentaires :\n",
    "\n",
    "- la cause (colonne `cause`), ayant trois modalités : `PERTURBATION`, `TRAVAUX`, et `INFORMATION`\n",
    "- le niveau d'incident (colonne `severity`) ayant trois modalités : `PERTURBEE`, `BLOQUANTE`, et `INFORMATION`\n",
    "- le type d'object impacté (colonne `object_type`) ayant quatre modalités : `line`, `stop_point`, `network`, et `stop_area`\n",
    "- le mode de transport (colonne `line_mode`) ayant six modalités : `Bus`, `Tramway`, `Metro`, `RapidTransit`, `LocalTrain`, et `Funicular`\n",
    "\n",
    "Pour cela, notre solution finale (après plusieurs itérations) consiste à utiliser un modèle d'embedding spécialisé en \n",
    "français, _CamemBERT_ (voir le [papier](https://arxiv.org/abs/1911.03894) associé). Nous utilisons précisément la \n",
    "version pré-entrainée `camembert-base` (voir la \n",
    "[carte du modèle sur HuggingFace](https://huggingface.co/almanach/camembert-base)), de 110 millions de paramètres. \n",
    "Elle est entraînée sur le corpus OSCAR, jeu de 138 GO de données générales en français, jeu qui ressemble plus à notre\n",
    "situation que celui de _FlauBERT_ (voir [carte HuggingFace](https://huggingface.co/flaubert/flaubert_base_uncased)), \n",
    "autre modèle d'embedding spécialisé en français que nous aurions pu utiliser.\n",
    "\n",
    "Ce modèle est basé sur l'architecture _RoBERTa_ (voir le [papier](https://arxiv.org/pdf/1907.11692) associé), une \n",
    "itération un peu plus moderne de _BERT_ (voir le [papier](https://arxiv.org/abs/1810.04805) associé), et calcule donc \n",
    "un embedding du message d'alerte. Nous le combinons avec une tête de régression assez simple, à savoir un perceptron \n",
    "multi-couche assez simple (simplement deux-couches et une ReLU), chargée de prédire en même temps la durée de la \n",
    "perturbation, mais aussi les données catégoriques associées. Cette approche permet donc (en théorie) de faire \n",
    "apprendre au modèle à reconnaître les différents types d'incidents, en plus de prédire leur durée.\n",
    "\n",
    "Pour cela, on ne garde que les incidences se déroulant sur un jour au maximum (c'est principalement ce qui \n",
    "nous intéresse), les analyses préliminaires ayant montré qu'il pouvait sinon y avoir beaucoup de valeurs préliminaires. \n",
    "La durée est représentée sous deux catégories flottantes (heures et minutes), traitées en amont avec une normalisation \n",
    "robuste, et évaluées par une erreur quadratique moyenne. Les données catégoriques sont représentées avec un encodage one-hot, et évaluées par une perte d'entropie croisée binaire. Le modèle est entraînée avec une perte correspondant à la somme de ces deux métriques, et l'optimisateur AdamW (Adam étant utilisé par CamemBERT et RoBERTa, voir les papiers associés, et AdamW semblant en être une version légèrement supérieure).\n",
    "\n",
    "Le diagramme ci-dessous résume le modèle :\n",
    "\n",
    "![diagramme récapitulatif](images/modele.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce146-aa60-4e11-ae12-23ec12784301",
   "metadata": {},
   "source": [
    "### Traitement des données\n",
    "\n",
    "On commence par importer le jeu de données des liens objets-perturbations, générées préalablement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d297a77366e1b17e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T05:32:01.298521Z",
     "start_time": "2024-12-30T05:32:01.222951Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_disruptions = pd.read_feather(\"data/objects_disruptions.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be5138-ab58-4bf4-ada8-df3f6d1be907",
   "metadata": {},
   "source": [
    "Le champ `message` issu de l'api est formaté en HTML et non pas en texte brut. Avec le module `html2text`, on le convertit donc en texte plein et on l'assemble dans une colonne avec la colonne `title` (titre du message). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eedc1b7-6049-4fc2-a979-6fc761f7a48d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from html2text import html2text\n",
    "\n",
    "# Prétraitement des données\n",
    "df_disruptions[\"text\"] = df_disruptions[\"title\"] + \"\\n\\n\" + df_disruptions[\"message\"].apply(html2text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fac58-a48b-4176-b88f-b8539403f10a",
   "metadata": {},
   "source": [
    "On convertit les colonnes `begin` et `end` (début et fin de la perturbation) de leur champ texte aux formats `datetime[*]` adaptés. On calcule la différence dans une colonne `delta` (format `timedelta[*]`), qu'on sépare en trois colonnes `duration_days`, `duration_hours` et `duration_minutes` correspondant respectivement à la durée de l'incident en jours, en heures et minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395cdf82-9acc-4940-9291-35dffb01c4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# les colonnes begin et end sont au format YYYYMMDDThhmmss donc on les parse avec le format %Y%m%dT%H%M%S.\n",
    "# Il ne devrait pas y avoir d'exception, mais dans le doute errors=\"coerce\" permet d'avoir des NaN.\n",
    "df_disruptions[\"begin\"] = pd.to_datetime(df_disruptions[\"begin\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\")\n",
    "df_disruptions[\"end\"] = pd.to_datetime(df_disruptions[\"end\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\")\n",
    "df_disruptions[\"delta\"] = df_disruptions[\"end\"] - df_disruptions[\"begin\"]\n",
    "\n",
    "# Calcul des jours, heures et minutes à partir de la colonne delta\n",
    "df_disruptions[\"duration_days\"] = df_disruptions[\"delta\"].dt.days\n",
    "df_disruptions[\"duration_hours\"] = df_disruptions[\"delta\"].dt.seconds // 3600  # Heures restantes\n",
    "df_disruptions[\"duration_minutes\"] = (df_disruptions[\"delta\"].dt.seconds % 3600) // 60  # Minutes restantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a0709-b2e3-4ce8-b00a-d1a07d6b6b6b",
   "metadata": {},
   "source": [
    "On peut alors vérifier la distribution des durées en jour en regardant celle de `duration_days` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9886686d-4426-48cb-b0cb-632e28a6f822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1       0.0\n",
       "0.2       0.0\n",
       "0.3       0.0\n",
       "0.4       0.0\n",
       "0.5       0.0\n",
       "0.6       0.0\n",
       "0.7       0.0\n",
       "0.8       0.0\n",
       "0.9      23.0\n",
       "1.0    5113.0\n",
       "Name: duration_days, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disruptions[\"duration_days\"].quantile([n/10 for n in range(1, 11)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339c52d-787d-4895-8f06-203eff233e95",
   "metadata": {},
   "source": [
    "On observe que plus de $80 \\%$ des pertubations se déroulent sur moins d'un jour. On filtre donc dans un nouveau dataframe `df_disruptions_filtered` ces perturbations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4107af38-4afe-45cd-ae0c-be7cb79fae78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_disruptions_filtered = df_disruptions[df_disruptions[\"duration_days\"] < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489cddba-60e1-4279-8cb1-e7e810d9d6ba",
   "metadata": {},
   "source": [
    "On va ensuite appliquer une normalisation robuste des colonnes `duration_hours` et `duration_hours` sur le jeu qu'on vient de filtrer. On la calcule avec\n",
    "\n",
    "$$ X_{i,\\text{robuste}} = \\dfrac{X_i - X_{q,50}}{X_{q,75} - X_{q, 25}} $$\n",
    "\n",
    "où $X_{q,25}, X_{q,50}, X_{q,75}$ sont le premier, deuxième (médianne) et troisième quartile de $X$ (il existe des versions où on soustrait plutôt le premier quartile). Cette normalisation permet de tenir compte des valeurs aberrantes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a6d9e2-2d87-4bbd-91be-9e6d263ab62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_710/496045166.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_disruptions_filtered[\"hours\"] = (df_disruptions_filtered[\"duration_hours\"] - quantiles_hours[0.5]) / iqr_hours\n",
      "/tmp/ipykernel_710/496045166.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_disruptions_filtered[\"minutes\"] = (df_disruptions_filtered[\"duration_minutes\"] - quantiles_minutes[0.5]) / iqr_minutes\n"
     ]
    }
   ],
   "source": [
    "# Calcul des quartiles\n",
    "quantiles_hours = df_disruptions_filtered[\"duration_hours\"].quantile([0.25, 0.5, 0.75])\n",
    "quantiles_minutes = df_disruptions_filtered[\"duration_minutes\"].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Calcul de l'écart inter-quartile\n",
    "iqr_hours = quantiles_hours[0.75] - quantiles_hours[0.25]\n",
    "iqr_minutes = quantiles_minutes[0.75] - quantiles_minutes[0.25]\n",
    "\n",
    "# Normalisation\n",
    "df_disruptions_filtered[\"hours\"] = (df_disruptions_filtered[\"duration_hours\"] - quantiles_hours[0.5]) / iqr_hours\n",
    "df_disruptions_filtered[\"minutes\"] = (df_disruptions_filtered[\"duration_minutes\"] - quantiles_minutes[0.5]) / iqr_minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ef964-84b0-4abb-b0e8-c5b9ca0b4e94",
   "metadata": {},
   "source": [
    "On calcule ensuite un DataFrame `df` final, avec l'encodage one-hot des colonnes `cause`, `severity`, `object_type`, `line_mode`. On enregistre dans `targets` les colonnes des cibles, et on convertit ces colonnes au format flottant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabccd38-1a50-4d36-932b-37909936ba28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One-hot encoding des colonnes catégoriques\n",
    "df = pd.get_dummies(\n",
    "    df_disruptions_filtered[[\"text\", \"hours\", \"minutes\", \"cause\", \"severity\", \"object_type\", \"line_mode\"]],\n",
    "    columns=[\"cause\", \"severity\", \"object_type\", \"line_mode\"]\n",
    ")\n",
    "\n",
    "# On note les colonnes des cibles\n",
    "targets = [k for k in df.columns if k != \"text\"]\n",
    "\n",
    "# Conversion des colonnes cibles en flottants\n",
    "for col in targets[1:]:\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fc09a-8830-49f5-bfca-f18f1b3bc200",
   "metadata": {},
   "source": [
    "On sépare ce DataFrame en un jeu d'entraînement (80%) et de test (20%), en l'ayant mélangé. Par soucis de reproductibilité, le `random_state` a été fixé à `123456789`. On sépare les données d'entrées (nommées $X$ par convention), qui sont la colonne `text` et les cibles (nommées $y$ par convention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8642e370-9e6c-430c-9f8e-dc8899b6a22a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "\n",
    "# Diviser le DataFrame \n",
    "df_shuffled = df.sample(frac=1, random_state=123456789).reset_index(drop=True)  # Mélanger\n",
    "train_df = df_shuffled.iloc[:int(len(df) * train_size)]\n",
    "test_df = df_shuffled.iloc[int(len(df) * train_size):]\n",
    "\n",
    "# Texte (entrée) et cibles\n",
    "X_train = train_df[\"text\"].values.tolist()\n",
    "X_test = test_df[\"text\"].values.tolist()\n",
    "y_train = train_df[targets].values\n",
    "y_test = test_df[targets].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f775de7-93e3-4cd9-b00d-44f64889d439",
   "metadata": {},
   "source": [
    "On va ensuite tokeniser les messages en utilisant le tokenizer de CamemBERT. Voir la [documentation](https://huggingface.co/docs/transformers/en/model_doc/camembert#transformers.CamembertTokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c5cda0c-7f76-4d42-ae23-67b55292da4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "# Chargement le tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Tokenisation des X d'entraînement et de test\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train)\n",
    "test_encodings = tokenize_texts(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf13bec-4ec4-4b0d-8cca-00c2e1c0114b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entraînement et évaluation\n",
    "\n",
    "Pour l'entraînement et l'évaluation, on utilise le module `Pytorch` et les utilitaires adaptés. On crée donc d'abord un objet `RegressionDataset` héritant du `Dataset` pytorch, qu'on instancie pour le jeu d'entraînement et d'évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c1214ec-fb31-4619-9e99-d0eb00584ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Initialise un Dataset torch, avec deux atributs\n",
    "        :param encodings: encodings issus du tokenizer CamemBERT.\n",
    "        :param labels: cibles (ici multiples, flotants pour la durée et les cibles booléenes)\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Méthode classique de torch, on formatte un objet pour notre cas d'utilisation.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            key: val[idx] for key, val in self.encodings.items()\n",
    "        } | {\"labels\": torch.tensor(self.labels[idx], dtype=torch.float32)}\n",
    "\n",
    "train_dataset = RegressionDataset(train_encodings, y_train)\n",
    "test_dataset = RegressionDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b685db1-7396-419d-9df9-3e44426a752b",
   "metadata": {},
   "source": [
    "On utilise des `DataLoader` pour charger progressivement ces datasets lors de l'entraînement et de l'évaluation. Le `batch_size` peut être modifié (les valeurs jusqu'à 128 semblent fonctionner sur le SSP Cloud), mais ne change visiblement pas la vitesse d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6e7609-9354-48db-b001-6ade62888d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03307a67-ffaa-4a09-8d1e-546cb8b9cab9",
   "metadata": {},
   "source": [
    "On crée un objet `CamembertForRegression`, qu'on instancie, correspondant au modèle décrit plus haut. Pour rappel :\n",
    "\n",
    "![modele](images/modele.png)\n",
    "\n",
    "**NB :** Le modèle est chargé directement sur CUDA. Si on lance l'entraînement sur cpu, il faudrait faire différement (par exemple utiliser la variable device plus bas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df2a36af-7144-4de5-b354-403bfca8dad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import CamembertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CamembertForRegression(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        \"\"\"\n",
    "        Initialise le modèle CamemBERT pour la régression.\n",
    "        :param num_outputs: Nombre de sorties de la régression\n",
    "        \"\"\"\n",
    "        super(CamembertForRegression, self).__init__() \n",
    "        \n",
    "        # Le modèle est basé sur une version pré-entraînée de CamemBERT,\n",
    "        # en l'ocurrence ici camembert-base (110 millions de paramètres, entraîné sur OSCAR),\n",
    "        # voir https://huggingface.co/almanach/camembert-base. On pourrait sinon\n",
    "        # utiliser camembert-large (335 millions de paramètres).\n",
    "        self.model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "        \n",
    "        # On ajoute un perceptron multi-couche comme tête de régression, laquelle transforme\n",
    "        # les embeddings produits par CamemBERT (de taille self.model.config.hidden_size)\n",
    "        # en valeurs pour la tâche de régression (de taille num_outputs)\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(self.model.config.hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Définition de la propagation avant (forward).\n",
    "        :param input_ids: Identifiants des tokens (token IDs) du texte.\n",
    "        :param attention_mask: Masque pour ignorer les positions de padding.\n",
    "        :return: Les prédictions issues de la tête de régression.\n",
    "        \"\"\"\n",
    "        # Passe les données dans le modèle CamemBERT\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Récupère l'embedding du token [CLS] censé être l'embedding résumé du texte d'entrée\n",
    "        # voir https://arxiv.org/abs/1810.04805\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Applique la tête de régression pour produire les prédictions finales\n",
    "        return self.regression_head(cls_embedding)\n",
    "\n",
    "\n",
    "# Instancie le modèle\n",
    "model = CamembertForRegression(num_outputs=y_train.shape[1]).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846f45a-4dbf-4d58-8600-22b3504457c8",
   "metadata": {},
   "source": [
    "On instancie un optimisateur AdamW. Le taux d'apprentisage choisi ($3 \\cdot 10^{-5}$) semble bien fonctionner sur quelques tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3128b3a9-bdc4-4661-97ec-3fa29b1f3d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e65fd1-c20f-4793-9e33-e5a3d5909f28",
   "metadata": {},
   "source": [
    "On crée un objet `MultiTaskLoss` correspondant à la fonction de perte décrit plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25fa043-a7ac-4be8-8943-293867919628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        # Séparer les cibles continues et binaires\n",
    "        duration_preds = predictions[:, :3]  # Les trois premières colonnes : jours, heures, minutes\n",
    "        duration_labels = labels[:, :3]\n",
    "\n",
    "        binary_preds = predictions[:, 3:]  # Autres cibles booléennes\n",
    "        binary_labels = labels[:, 3:]\n",
    "\n",
    "        # Calcul des pertes\n",
    "        mse_loss = self.mse(duration_preds, duration_labels)\n",
    "        bce_loss = self.bce(binary_preds, binary_labels)\n",
    "\n",
    "        # Retourner les pertes sous forme de scalaires\n",
    "        return mse_loss.item(), bce_loss.item(), (mse_loss + bce_loss).item()\n",
    "    \n",
    "loss_fn = MultiTaskLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e83df-61de-4f09-91bd-df8c23ac1e88",
   "metadata": {},
   "source": [
    "Enfin, on lance l'entraînement sur 7 epochs. Pour chaque epoch, on calcule la perte d'entraînement et d'évaluation, ainsi que l'erreur quadratique moyenne sur les minutes et les secondes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dee3819-c7b3-45e4-a34d-962b57824a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3785/3785 [14:39<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.0745\n",
      "Epoch 1, Training MSE Loss: 0.3734\n",
      "Epoch 1, Training BCE Loss: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [03:24<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1.0743\n",
      "Epoch 1, Validation MSE Loss: 0.3739\n",
      "Epoch 1, Validation BCE Loss: 0.7004\n",
      "Epoch 1, MSE Hours: 0.7226\n",
      "Epoch 1, MSE Minutes: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3785/3785 [14:39<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 1.0746\n",
      "Epoch 2, Training MSE Loss: 0.3734\n",
      "Epoch 2, Training BCE Loss: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [03:24<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation Loss: 1.0743\n",
      "Epoch 2, Validation MSE Loss: 0.3739\n",
      "Epoch 2, Validation BCE Loss: 0.7004\n",
      "Epoch 2, MSE Hours: 0.7226\n",
      "Epoch 2, MSE Minutes: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3785/3785 [14:41<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 1.0746\n",
      "Epoch 3, Training MSE Loss: 0.3734\n",
      "Epoch 3, Training BCE Loss: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [03:24<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation Loss: 1.0743\n",
      "Epoch 3, Validation MSE Loss: 0.3739\n",
      "Epoch 3, Validation BCE Loss: 0.7004\n",
      "Epoch 3, MSE Hours: 0.7226\n",
      "Epoch 3, MSE Minutes: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3785/3785 [09:25<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 1.0746\n",
      "Epoch 4, Training MSE Loss: 0.3734\n",
      "Epoch 4, Training BCE Loss: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [01:36<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation Loss: 1.0743\n",
      "Epoch 4, Validation MSE Loss: 0.3739\n",
      "Epoch 4, Validation BCE Loss: 0.7004\n",
      "Epoch 4, MSE Hours: 0.7226\n",
      "Epoch 4, MSE Minutes: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3785/3785 [06:52<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 1.0746\n",
      "Epoch 5, Training MSE Loss: 0.3734\n",
      "Epoch 5, Training BCE Loss: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [01:36<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Validation Loss: 1.0743\n",
      "Epoch 5, Validation MSE Loss: 0.3739\n",
      "Epoch 5, Validation BCE Loss: 0.7004\n",
      "Epoch 5, MSE Hours: 0.7226\n",
      "Epoch 5, MSE Minutes: 0.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entraînement:   9%|██████████████                                                                                                                                           | 347/3785 [00:37<06:14,  9.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     16\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 18\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m mse_loss, bce_loss, loss \u001B[38;5;241m=\u001B[39m loss_fn(predictions, labels)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Convertir 'loss' en tenseur avant d'appeler backward\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[13], line 36\u001B[0m, in \u001B[0;36mCamembertForRegression.forward\u001B[0;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03mDéfinition de la propagation avant (forward).\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m:param input_ids: Identifiants des tokens (token IDs) du texte.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;03m:param attention_mask: Masque pour ignorer les positions de padding.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03m:return: Les prédictions issues de la tête de régression.\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Passe les données dans le modèle CamemBERT\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Récupère l'embedding du token [CLS] censé être l'embedding résumé du texte d'entrée\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# voir https://arxiv.org/abs/1810.04805\u001B[39;00m\n\u001B[1;32m     40\u001B[0m cls_embedding \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:888\u001B[0m, in \u001B[0;36mCamembertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    879\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    881\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    882\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    883\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    886\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    887\u001B[0m )\n\u001B[0;32m--> 888\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    894\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    896\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    897\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    898\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    900\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    901\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:538\u001B[0m, in \u001B[0;36mCamembertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    527\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    528\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    529\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    535\u001B[0m         output_attentions,\n\u001B[1;32m    536\u001B[0m     )\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 538\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    542\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    543\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    544\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    548\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    549\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:427\u001B[0m, in \u001B[0;36mCamembertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    417\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    424\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    425\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    426\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 427\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:354\u001B[0m, in \u001B[0;36mCamembertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    346\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    352\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    353\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 354\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    363\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    364\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:233\u001B[0m, in \u001B[0;36mCamembertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    231\u001B[0m     value_layer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([past_key_value[\u001B[38;5;241m1\u001B[39m], value_layer], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 233\u001B[0m     key_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose_for_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    234\u001B[0m     value_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue(hidden_states))\n\u001B[1;32m    236\u001B[0m query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001B[0;32m~/ai/.venv/lib/python3.10/site-packages/transformers/models/camembert/modeling_camembert.py:199\u001B[0m, in \u001B[0;36mCamembertSelfAttention.transpose_for_scores\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    197\u001B[0m new_x_shape \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_attention_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_head_size)\n\u001B[1;32m    198\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(new_x_shape)\n\u001B[0;32m--> 199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(7):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_mse_loss = 0\n",
    "    train_bce_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, \"Entraînement\"):\n",
    "        optimizer.zero_grad() #On initialise l'optimisateur\n",
    "    \n",
    "        # on envoie les données sur CUDA\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # on infère avec le modèle    \n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        \n",
    "        # on calcule la perte\n",
    "        mse_loss, bce_loss, loss = loss_fn(predictions, labels)\n",
    "    \n",
    "        # on converti en tenseur\n",
    "        loss_tensor = torch.tensor(loss, requires_grad=True).to(device)\n",
    "        \n",
    "        # rétropropagation\n",
    "        loss_tensor.backward()\n",
    "        \n",
    "        # cliping des gradients (nous n'avons pas testé d'autres valeurs que 1 pour max_norm, par manque de temps)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        # mise à jour des poids\n",
    "        optimizer.step()\n",
    "        \n",
    "        # incrémentation des pertes pour l'affichage sur l'epoch    \n",
    "        train_loss += loss\n",
    "        train_mse_loss += mse_loss\n",
    "        train_bce_loss += bce_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Training MSE Loss: {train_mse_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Training BCE Loss: {train_bce_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # enregistrement des poids du modèle à l'epoch\n",
    "    torch.save(model.state_dict(), \"checkpoint/epoch_\" + str(epoch + 1) + \".pt\")\n",
    "\n",
    "    # Evaluation. Le code est sensiblement le même.\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mse_loss = 0\n",
    "    val_bce_loss = 0\n",
    "\n",
    "    # Variables pour accumuler les cibles et prédictions\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            mse_loss, bce_loss, loss = loss_fn(predictions, labels)\n",
    "\n",
    "            val_loss += loss\n",
    "            val_mse_loss += mse_loss\n",
    "            val_bce_loss += bce_loss\n",
    "\n",
    "            # stockages les cibles et prédictions pour les MSE séparées\n",
    "            all_labels.append(labels[:, :2])  # Garder uniquement heures et minutes\n",
    "            all_predictions.append(predictions[:, :2])\n",
    "\n",
    "    # Concatenation des résultats sur tous les batches\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    # Calcul des MSE pour chaque cible\n",
    "    mse_hours = torch.mean((all_labels[:, 0] - all_predictions[:, 0]) ** 2).item()\n",
    "    mse_minutes = torch.mean((all_labels[:, 1] - all_predictions[:, 1]) ** 2).item()\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Validation MSE Loss: {val_mse_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Validation BCE Loss: {val_bce_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, MSE Hours: {mse_hours:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, MSE Minutes: {mse_minutes:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83704c7-1d26-4d53-b233-3105b67d14c0",
   "metadata": {},
   "source": [
    "Sur les cinq premiers epochs, on observe que les pertes d'entrainement et de validation sont extrêmement stables, et ce, dès le premier epoch. L'interprétation est assez claire : le modèle n'apprend plus. **On arrête donc l'entraînement pendant le sixième epoch.**\n",
    "\n",
    "On enregistre dans un fichier checkpoint les valeurs de normalisation, pour pouvoir recalculer facilement les sorties du modèle en minutes et heures réelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c152555b-b561-414c-939d-08e7af1e2301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stockage des valeurs de normalisation et du nombre de sorties du modèle pour pouvoir le recharger\n",
    "normalization_values = {\n",
    "    \"quantiles_hours\": quantiles_hours.to_dict(),\n",
    "    \"quantiles_minutes\": quantiles_minutes.to_dict(),\n",
    "    \"iqr_hours\": iqr_hours,\n",
    "    \"iqr_minutes\": iqr_minutes,\n",
    "    \"num_outputs\": y_train.shape[1]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(\"checkpoint/normalization.json\", \"w\") as f:\n",
    "    json.dump(normalization_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d553d44-2126-42d5-8f21-a6f125412981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
