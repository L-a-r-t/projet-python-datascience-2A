{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "_Note préliminaire :_ Ce notebook a été éxécuté avec l'environnement jupyter-pytorch-gpu. De nombreuses dépendances y sont présentes et ne sont pas explicitement installées dans ce notebook. La cellule ci-dessous installe les dépendances supplémentaires. Du fait de l'installation de SentencePiece, il faut **impérativement redémarrer** le kernel jupyter après.",
   "id": "2de4299901b87fe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install pandas numpy html2text torch transformers tqdm SentencePiece",
   "id": "ed9edfd791873af6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Entrainement et évaluation du modèle\n",
    "\n",
    "Ce notebook détaille l'entraînement du modèle."
   ],
   "id": "8641abec7ab61f62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "df_disruptions = pd.read_feather(\"data/objects_disruptions.feather\")"
   ],
   "id": "6716879b7021d812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from html2text import html2text\n",
    "\n",
    "# Prétraitement des données\n",
    "df_disruptions[\"text\"] = df_disruptions[\"title\"] + \"\\n\\n\" + df_disruptions[\"message\"].apply(html2text)\n",
    "df_disruptions[\"begin\"] = pd.to_datetime(df_disruptions[\"begin\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\")\n",
    "df_disruptions[\"end\"] = pd.to_datetime(df_disruptions[\"end\"], format=\"%Y%m%dT%H%M%S\", errors=\"coerce\")\n",
    "\n",
    "# Calcul des jours, heures et minutes à partir de duration\n",
    "df_disruptions[\"duration_days\"] = (df_disruptions[\"end\"] - df_disruptions[\"begin\"]).dt.days\n",
    "df_disruptions[\"duration_hours\"] = (df_disruptions[\"end\"] - df_disruptions[\"begin\"]).dt.seconds // 3600  # Heures restantes\n",
    "df_disruptions[\"duration_minutes\"] = ((df_disruptions[\"end\"] - df_disruptions[\"begin\"]).dt.seconds % 3600) // 60  # Minutes restantes"
   ],
   "id": "571c47d02f84df2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalisation robuste des durées (prise en compte des outliers)\n",
    "quantiles_days = df_disruptions[\"duration_days\"].quantile([0.25, 0.75])\n",
    "quantiles_hours = df_disruptions[\"duration_hours\"].quantile([0.25, 0.75])\n",
    "quantiles_minutes = df_disruptions[\"duration_minutes\"].quantile([0.25, 0.75])\n",
    "\n",
    "# Calcul de l'IQR (Interquartile Range)\n",
    "iqr_days = quantiles_days[0.75] - quantiles_days[0.25]\n",
    "iqr_hours = quantiles_hours[0.75] - quantiles_hours[0.25]\n",
    "iqr_minutes = quantiles_minutes[0.75] - quantiles_minutes[0.25]\n",
    "\n",
    "# Normalisation par IQR\n",
    "df_disruptions[\"duration_days_normalized\"] = (df_disruptions[\"duration_days\"] - quantiles_days[0.25]) / iqr_days\n",
    "df_disruptions[\"duration_hours_normalized\"] = (df_disruptions[\"duration_hours\"] - quantiles_hours[0.25]) / iqr_hours\n",
    "df_disruptions[\"duration_minutes_normalized\"] = (df_disruptions[\"duration_minutes\"] - quantiles_minutes[0.25]) / iqr_minutes"
   ],
   "id": "6e2fa2351b5441b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-hot encoding des colonnes catégoriques, avec drop_first pour économiser des colonnes.\n",
    "df = pd.get_dummies(\n",
    "    df_disruptions[[\"text\", \"duration_days\", \"duration_hours\", \"duration_minutes\", \"cause\", \"severity\", \"object_type\", \"line_mode\"]],\n",
    "    columns=[\"cause\", \"severity\", \"object_type\", \"line_mode\"],\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "targets = [k for k in df.columns if k != \"text\"]\n",
    "\n",
    "# Conversion des colonnes booléennes en float\n",
    "for col in targets[1:]:\n",
    "    df[col] = df[col].astype(float)"
   ],
   "id": "4db4e0561aa55f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_size = 0.2  # 20% pour le test\n",
    "train_size = 1 - test_size\n",
    "\n",
    "# Diviser le DataFrame\n",
    "df_shuffled = df.sample(frac=1, random_state=123456789).reset_index(drop=True)  # Mélanger\n",
    "train_df = df_shuffled.iloc[:int(len(df) * train_size)]\n",
    "test_df = df_shuffled.iloc[int(len(df) * train_size):]\n",
    "\n",
    "# Texte (entrée) et cibles\n",
    "X_train = train_df[\"text\"].values.tolist()\n",
    "X_test = test_df[\"text\"].values.tolist()\n",
    "y_train = train_df[targets].values\n",
    "y_test = test_df[targets].values\n"
   ],
   "id": "2f9fcd4b4a280b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import CamembertTokenizer\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Tokeniser les ensembles d'entraînement et de test\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train)\n",
    "test_encodings = tokenize_texts(X_test)"
   ],
   "id": "a47f89324b694a42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: val[idx] for key, val in self.encodings.items()\n",
    "        } | {\"labels\": torch.tensor(self.labels[idx], dtype=torch.float32)}\n",
    "\n",
    "train_dataset = MultiTaskDataset(train_encodings, y_train)\n",
    "test_dataset = MultiTaskDataset(test_encodings, y_test)"
   ],
   "id": "843aa8f7c84ca0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ],
   "id": "1df56d61ba2de3ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import CamembertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class CamembertForMultiTask(nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(CamembertForMultiTask, self).__init__()\n",
    "        self.model = CamembertModel.from_pretrained(\"camembert-base\")\n",
    "        self.regression_head = nn.Linear(self.model.config.hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Utilise [CLS]\n",
    "        return self.regression_head(cls_embedding)\n",
    "\n",
    "# Instancie le modèle\n",
    "model = CamembertForMultiTask(num_outputs=y_train.shape[1]).to(\"cuda\")\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, predictions, labels):\n",
    "        # Séparer les cibles continues et binaires\n",
    "        duration_preds = predictions[:, :3]  # Les trois premières colonnes : jours, heures, minutes\n",
    "        duration_labels = labels[:, :3]\n",
    "\n",
    "        binary_preds = predictions[:, 3:]  # Autres cibles booléennes\n",
    "        binary_labels = labels[:, 3:]\n",
    "\n",
    "        # Calcul des pertes\n",
    "        mse_loss = self.mse(duration_preds, duration_labels)\n",
    "        bce_loss = self.bce(binary_preds, binary_labels)\n",
    "\n",
    "        # Retourner les pertes sous forme de scalaires\n",
    "        return mse_loss.item(), bce_loss.item(), (mse_loss + bce_loss).item()"
   ],
   "id": "21f0c42921e1a94d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = MultiTaskLoss()"
   ],
   "id": "22cfb27525bcf625"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(7):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_mse_loss = 0\n",
    "    train_bce_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        mse_loss, bce_loss, loss = loss_fn(predictions, labels)\n",
    "    \n",
    "        # Convertir 'loss' en tenseur avant d'appeler backward\n",
    "        loss_tensor = torch.tensor(loss, requires_grad=True).to(device)\n",
    "        loss_tensor.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss\n",
    "        train_mse_loss += mse_loss\n",
    "        train_bce_loss += bce_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Training MSE Loss: {train_mse_loss / len(train_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Training BCE Loss: {train_bce_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"checkpoint/epoch_\" + str(epoch) + \".pt\")\n",
    "\n",
    "    # Calcul et affichage de la validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mse_loss = 0\n",
    "    val_bce_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            mse_loss, bce_loss, loss = loss_fn(predictions, labels)\n",
    "    \n",
    "            val_loss += loss\n",
    "            val_mse_loss += mse_loss\n",
    "            val_bce_loss += bce_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Validation MSE Loss: {val_mse_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}, Validation BCE Loss: {val_bce_loss / len(test_loader):.4f}\")"
   ],
   "id": "d0423ed8c82c680a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3cd6ea436b3adade"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
