{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération du dataset pour l'analyse de données\n",
    "\n",
    "Les données issues du scraping de l'API IDF Mobilités ne constituent pas un jeu de données en tant que tel. Le rôle de ce notebook est de passer du des données brutes à un jeu de données exploitable par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir plus de flexibilité pour mener notre analyse, nous ne travaillerons pas seulement avec un dataset combinant les données sur les perturbations et les lignes affectées, nous garderons aussi de côté les données propres aux perturbations et aux lignes séparées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.config import fs, ROOT\n",
    "\n",
    "paths = fs.ls(ROOT)\n",
    "\n",
    "all_results = []\n",
    "all_disruptions = []\n",
    "all_lines = []\n",
    "\n",
    "for file_path in paths:\n",
    "    with open(file_path, 'r', encoding='ascii') as f:\n",
    "        raw_data = f.read()\n",
    "        data = json.loads(raw_data)\n",
    "        \n",
    "        last_updated = data.get('lastUpdatedDate')\n",
    "        disruptions = data.get('disruptions', [])\n",
    "        lines = data.get('lines', [])\n",
    "        \n",
    "        all_results.append({\n",
    "            'lastUpdatedDate': last_updated,\n",
    "            'disruptions': disruptions,\n",
    "            'lines': lines\n",
    "        })\n",
    "        \n",
    "        for d in disruptions:\n",
    "            all_disruptions.append({\n",
    "                'disruption_id': d.get('id'),\n",
    "                'applicationPeriods': d.get('applicationPeriods'),\n",
    "                'lastUpdate': d.get('lastUpdate'),\n",
    "                'cause': d.get('cause'),\n",
    "                'severity': d.get('severity'),\n",
    "                'title': d.get('title'),\n",
    "                'message': d.get('message'),\n",
    "                'file_lastUpdatedDate': last_updated\n",
    "            })\n",
    "        \n",
    "        for l in lines:\n",
    "            all_lines.append({\n",
    "                'line_id': l.get('id'),\n",
    "                'name': l.get('name'),\n",
    "                'shortName': l.get('shortName'),\n",
    "                'mode': l.get('mode'),\n",
    "                'networkId': l.get('networkId'),\n",
    "                'impactedObjects': l.get('impactedObjects', []),\n",
    "                'file_lastUpdatedDate': last_updated\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du fait de perturbations pouvant être de longue durée (travaux...), nos données brutes comportent de nombreux duplicats que nous allons devoir traiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(all_results)\n",
    "df_disruptions = pd.DataFrame(all_disruptions)\n",
    "df_lines = pd.DataFrame(all_lines)\n",
    "\n",
    "print(\"Total disruptions (before):\", len(df_disruptions))\n",
    "print(\"Total lines (before):\", len(df_lines))\n",
    "print(\"Total results (before):\", len(df_results))\n",
    "\n",
    "df_results = df_results.drop_duplicates(subset=['lastUpdatedDate'])\n",
    "df_disruptions = df_disruptions.drop_duplicates(subset=['disruption_id'])\n",
    "df_lines = df_lines.drop_duplicates(subset=['line_id'])\n",
    "\n",
    "print(\"Total disruptions (after):\", len(df_disruptions))\n",
    "print(\"Total lines (after):\", len(df_lines))\n",
    "print(\"Total results (after):\", len(df_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jointure entre les lignes et les perturbations\n",
    "\n",
    "line_disruption_links = []\n",
    "\n",
    "for idx, row in df_lines.iterrows():\n",
    "    impacted_objects = row['impactedObjects']\n",
    "    line_id = row['line_id']\n",
    "    if impacted_objects:\n",
    "        for obj in impacted_objects:\n",
    "            disruptions_ids = obj.get('disruptionsIds', [])\n",
    "            for d_id in disruptions_ids:\n",
    "                line_disruption_links.append({\n",
    "                    'line_id': line_id,\n",
    "                    'disruption_id': d_id\n",
    "                })\n",
    "\n",
    "df_line_disruption = pd.DataFrame(line_disruption_links)\n",
    "\n",
    "# df_line_disruption.merge(df_lines, on='line_id') \\\n",
    "#                   .merge(df_disruptions, left_on='disruption_id', right_on='disruption_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
